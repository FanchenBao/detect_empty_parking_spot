{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('parking_spott_detection': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "85b71a5044282bba278e646f4c6a0eeaa7f686b5c3246edbddf4b00b6c49dfbc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from typing import List\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "source": [
    "# Shared Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture frames from the video for training and testing purpose\n",
    "# From this tutorial:\n",
    "# https://www.tutorialexample.com/python-capture-images-from-video-by-frames-using-opencv-a-complete-guide/\n",
    "\n",
    "VIDEO_PATH = Path('video/original.mp4')\n",
    "FRAME_FOLDER_PATH = Path('data/video_frames')\n",
    "\n",
    "def capture_frames(\n",
    "    num_captures: int = 10,\n",
    "    frame_frequency: int = 100,\n",
    "    video_path=VIDEO_PATH,\n",
    "    frame_folder_path=FRAME_FOLDER_PATH,\n",
    "):\n",
    "    \"\"\"Capture frames from the given video at a given frame frequency.\n",
    "\n",
    "    Captured frames will be stored in the folder specified by `frame_folder_path`. Each image\n",
    "    is labeled as {index}.png\n",
    "\n",
    "    :param frame_frequency: The frequency at which frames are captured as image.\n",
    "    :param video_path: A Path object to the video.\n",
    "    :param frame_folder_path: A Path object to the folder where the captured frames will be stored.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        exit(0)\n",
    "    total_frame = 0\n",
    "    idx = 0\n",
    "    while idx < num_captures:\n",
    "        ret, frame = cap.read()\n",
    "        if ret is False:\n",
    "            break\n",
    "        total_frame += 1\n",
    "        if total_frame % frame_frequency == 0:\n",
    "            image_path = frame_folder_path.joinpath(f'{idx:02}.png')\n",
    "            cv2.imwrite(str(image_path), frame)\n",
    "            print(image_path)\n",
    "            idx += 1\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "def show_images(images, number_of_images, cmap=None, fig_size_base=(20, 10), file_name=''):\n",
    "    \"\"\"Show images in a column.\n",
    "\n",
    "    :param images: A numpy array of images. Each image must be converted to an array already.\n",
    "    :param number_of_images: number of images to show. All images will be shown in a column.\n",
    "    :param cmap: Color map. If the image is grey scale, pass 'grey', otherwise leave as None.\n",
    "    :param fig_size_base: The base scale of figure size. Each figure is considered to be 20\n",
    "        in width and 10 in height.\n",
    "    \"\"\"\n",
    "    if number_of_images > 1:\n",
    "        fig, axes = plt.subplots(\n",
    "            number_of_images,\n",
    "            1,\n",
    "            figsize=(fig_size_base[0], number_of_images * fig_size_base[1]),\n",
    "        )\n",
    "        for image, ax in zip(images, axes.flatten()):\n",
    "            # Since the channels in cv2 are in the order of BGR, in order to fit\n",
    "            # the more conventional RGB scheme for matplotlib, we convert the color\n",
    "            # before plotting. See this SO answer\n",
    "            # https://stackoverflow.com/a/39316695/9723036\n",
    "            ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), cmap=cmap)\n",
    "            ax.set_axis_off()\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(30, 30))\n",
    "        ax.imshow(cv2.cvtColor(images, cv2.COLOR_BGR2RGB), cmap=cmap)\n",
    "        ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    if file_name:\n",
    "        plt.savefig(file_name)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def select_color_then_gray_scale(image, color_bgr: List, plusminus: int = 40): \n",
    "    \"\"\"Select a specific color parts in the image.\n",
    "\n",
    "    The image is first masked for the target color, and then returned as\n",
    "    the gray scaled version.\n",
    "\n",
    "    :param image: The image object obtained from cv2.imread().\n",
    "    :param color_bgr: A sample BGR value, as a list, for the color to be\n",
    "        selected.\n",
    "    :param plusminus: A numeric value to add to or subtract from the color_bgr\n",
    "        to create a range when selecting color from the image. This value is\n",
    "        hardcoded in this function, but there are non-hard-coded method, as\n",
    "        explained here: https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/\n",
    "    :return: A new image with the given color selected and then gray-scaled.\n",
    "    \"\"\"\n",
    "    color = np.uint8(color_bgr)\n",
    "    mask = cv2.inRange(image, color - plusminus, color + plusminus)\n",
    "    return cv2.cvtColor(\n",
    "        cv2.bitwise_and(image, image, mask=mask),\n",
    "        cv2.COLOR_BGR2GRAY,\n",
    "    )\n",
    "\n",
    "\n",
    "def detect_edges(image, h_low: int, h_high: int):\n",
    "    \"\"\"Detect edges in a gray scaled image.\n",
    "\n",
    "    :param image: The image to have its edges detected.\n",
    "    :param h_low: The hysteresis lower threshold.\n",
    "    :param h_high: The hysteresis higher threshold.\n",
    "    :return: A new image with its edge detected.\n",
    "    \"\"\"\n",
    "    return cv2.Canny(image, h_low, h_high)\n",
    "\n",
    "\n",
    "def blackout(image, vertices):\n",
    "    \"\"\"Black out the part of the image enclosed by a polygon specified by vertices.\n",
    "\n",
    "    This is used to remove unnecessary cluter on the image and focus on the area\n",
    "    of interest. The vertices need to be manually created.\n",
    "\n",
    "    :param image: The image to perform the blackout.\n",
    "    \"\"\"\n",
    "    mask = np.zeros_like(image)\n",
    "    if len(mask.shape) == 2:\n",
    "        cv2.fillPoly(mask, vertices, 255)\n",
    "    else:\n",
    "        cv2.fillPoly(mask, vertices, (255,) * mask.shape[2])\n",
    "    return cv2.bitwise_and(image, mask)\n",
    "\n",
    "\n",
    "def focus(image, percent_vertices):\n",
    "    \"\"\"Focus on the polygon defined by the percent_vertices.\n",
    "    \n",
    "    Everything else will be blacked out.\n",
    "\n",
    "    :param image: The image to be focused.\n",
    "    :param percent_vertices: The coordinates of the polygon vertices in terms of\n",
    "        the percentage of the rows and columns. e.g. if a percent_vertices[0] =\n",
    "        [0.1, 0.8], that means the vertex is at 10% of the total row and 80% of\n",
    "        the total column, i.e. on the upper-right corner.\n",
    "    :return: The focused image after black out.\n",
    "    \"\"\"\n",
    "    r, c = image.shape[:2]\n",
    "    # Manuall create the vertices from percent_vertices\n",
    "    vertices = np.array(\n",
    "        [[[int(c * cp), int(r * rp)] for rp, cp in percent_vertices]],\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "    return blackout(image, vertices)\n",
    "\n",
    "\n",
    "def line_detection(\n",
    "    edge_image,\n",
    "    original_image,\n",
    "    vote_threshold,\n",
    "    min_line_length,\n",
    "    max_line_gap,\n",
    "):\n",
    "    \"\"\"Detect vertical lines that are the parking demarcations.\n",
    "\n",
    "    From the canny edge output, we run cv2.HoughLinesP to probabilistically\n",
    "    detect lines. We use vote_threshold, min_line_length, and max_line_gap\n",
    "    to control how strict the criteria are for line detection. Tuning these\n",
    "    variables is a manual labor. Each time a new set of lines are detected,\n",
    "    we add them to the original image for inspection. We also put in the\n",
    "    original image, right next to the vertical lines, the x and y coordinates.\n",
    "    We create two images, one with the x coordinate and the other the two\n",
    "    y coordinates. The purpose is to manually pickout the key markers of\n",
    "    each parking row such that we can compute the total parking lot map\n",
    "    from these key markers (e.g. edge of a parking row, height and width\n",
    "    of a parking spot, etc.)\n",
    "\n",
    "    Note that we have also a requirement that the line must be vertical.\n",
    "    This is achieved by the conditional check -1 <= x1 - x2 <= 1\n",
    "\n",
    "    :param edge_image: The image after the process of Canny edge detector.\n",
    "    :param original_image: The original image where the lines will be drawn.\n",
    "    :param vote_threshold: Min number of points on a line for the line to be\n",
    "        considered a line by the HoughLinesP.\n",
    "    :param min_line_length: Lines shorter than this value are discarded.\n",
    "    :param max_line_gap: Lines with a width larger than this value are discarded.\n",
    "    :return: Two copied original images with lines drawn on it. The first one labeled\n",
    "        by x coordinate and the second one y coordinate.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edge_image,\n",
    "        1,  # rho accuracy, unit pixel\n",
    "        np.pi / 180,  # theta accuracy, unit degree\n",
    "        vote_threshold,\n",
    "        minLineLength=min_line_length,\n",
    "        maxLineGap=max_line_gap,\n",
    "    )\n",
    "    lines = lines.reshape(-1, 4).tolist()\n",
    "    vert_lines = [tup for tup in lines if abs(tup[0] - tup[2]) <= 1]\n",
    "    copy_images = [np.copy(original_image), np.copy(original_image)]\n",
    "    for x1, y1, x2, y2 in vert_lines:  # x coord\n",
    "        cv2.line(copy_images[0], (x1, y1), (x2, y2), [0, 0, 255], 2)\n",
    "        cv2.putText(\n",
    "            copy_images[0],\n",
    "            str((x1 + x2) // 2),  # text\n",
    "            (x1, min(y1, y2)),  # lower left coordinates\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,  # font\n",
    "            0.3,  # font scale\n",
    "            (0, 255, 255),  # color\n",
    "        )\n",
    "    for x1, y1, x2, y2 in vert_lines:  # y coord\n",
    "        cv2.line(copy_images[1], (x1, y1), (x2, y2), [0, 0, 255], 2)\n",
    "        cv2.putText(  # line upper bound\n",
    "            copy_images[1],\n",
    "            str(min(y1, y2)),  # text\n",
    "            (x1, min(y1, y2)),  # lower left coordinates\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,  # font\n",
    "            0.3,  # font scale\n",
    "            (0, 255, 0),  # color\n",
    "        )\n",
    "        cv2.putText(  # line lower bound\n",
    "            copy_images[1],\n",
    "            str(max(y1, y2)),  # text\n",
    "            (x1, max(y1, y2)),  # lower left coordinates\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,  # font\n",
    "            0.3,  # font scale\n",
    "            (255, 0, 0),  # color\n",
    "        )\n",
    "    return np.array(copy_images)\n",
    "\n",
    "\n",
    "def draw_bounding_box(original_images, spot_dict_pickle_name: str):\n",
    "    \"\"\"Draw bounding boxes on the image to identiy how good the spot detection is.\n",
    "\n",
    "    :param original_images: The original unadultered images.\n",
    "    :param spot_dict_pickle_name: Name of the pickle file containing the coordinates\n",
    "        of all identified spots.\n",
    "    :return: Images with the bounding boxes drawn.\n",
    "    \"\"\"\n",
    "    copy_images = [np.copy(img) for img in original_images]\n",
    "    with open(spot_dict_pickle_name, 'rb') as f_obj:\n",
    "        spot_dict = pickle.load(f_obj)\n",
    "    for img in copy_images:\n",
    "        for x1, y1, x2, y2 in spot_dict.keys():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    return np.array(copy_images)\n",
    "\n",
    "\n",
    "def save_spot_images(original_images_ids, spot_dict_pickle_name: str, save_path: str = 'temp') -> None:\n",
    "    \"\"\"Save the spot image from each spot coordinate specified in spot_dict.\n",
    "\n",
    "    This is how we create all the training samples.\n",
    "\n",
    "    :param original_images_ids: The original unadultered images along with its ids.\n",
    "    :param spot_dict_pickle_name: Name of the pickle file containing the coordinates\n",
    "        of all identified spots.\n",
    "    \"\"\"\n",
    "    with open(spot_dict_pickle_name, 'rb') as f_obj:\n",
    "        spot_dict = pickle.load(f_obj)\n",
    "    for img, img_id in original_images_ids:\n",
    "        for spot_id, (x1, y1, x2, y2) in enumerate(spot_dict.keys()):\n",
    "            spot_img = img[y1:y2, x1:x2]\n",
    "            file_name = f'f{img_id:02}_s{spot_id:03}.png'\n",
    "            cv2.imwrite(f'{save_path}/{file_name}', spot_img)\n",
    "            print(f'{file_name} saved!')"
   ]
  },
  {
   "source": [
    "# Capture Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_frames()"
   ]
  },
  {
   "source": [
    "# Obtain All Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_paths = sorted(FRAME_FOLDER_PATH.glob('*.png'))\n",
    "# Use cv2 to read images. All images shall be read via cv2.\n",
    "all_images = np.array([\n",
    "    [cv2.imread(str(p)), i] for i, p in enumerate(all_image_paths)\n",
    "])"
   ]
  },
  {
   "source": [
    "# Show Sample Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_INDICES = np.array([3, 8])\n",
    "images = all_images[IMAGE_INDICES, 0]\n",
    "show_images(images, images.shape[0], file_name='images/original_images.png')\n",
    "print(f'Images shown are indices: {all_images[IMAGE_INDICES, 1]}')"
   ]
  },
  {
   "source": [
    "# Select White Color"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_bgr = [200, 200, 200]\n",
    "whites = np.array([select_color_then_gray_scale(img, white_bgr) for img in images])\n",
    "show_images(whites, whites.shape[0], file_name='images/white_selected.png')"
   ]
  },
  {
   "source": [
    "# Edge Detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_edges = np.array([detect_edges(img, 60, 150) for img in whites])\n",
    "show_images(white_edges, white_edges.shape[0], file_name='images/edge_detection.png')"
   ]
  },
  {
   "source": [
    "# Focus on Area of Interest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_vertices = np.array([  # manually acquired\n",
    "    [0.01, 0.20],\n",
    "    [0.01, 0.77],\n",
    "    [0.94, 0.77],\n",
    "    [0.94, 0.20],\n",
    "])\n",
    "\n",
    "focused_edges = np.array([focus(img, percent_vertices) for img in white_edges])\n",
    "show_images(focused_edges, focused_edges.shape[0], file_name='images/area_of_interest.png')"
   ]
  },
  {
   "source": [
    "# Line Detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use only the second image for line detection\n",
    "lines_images = line_detection(focused_edges[1], images[1], 50, 10, 5)\n",
    "show_images(lines_images, lines_images.shape[0], file_name='images/line_detection.png')"
   ]
  },
  {
   "source": [
    "# Manual Identification of All Potential Spots"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The row_xs and row_ys are acquired by inspecting the line_detection.png\n",
    "row_ys = np.array([  # the upper and lower y coordinates of each parking lot row\n",
    "    [35, 111],\n",
    "    [150, 237],\n",
    "    [287, 365],\n",
    "    [415, 492],\n",
    "    [544, 621],\n",
    "    [673, 751],\n",
    "    [803, 883],\n",
    "    [935, 1010],\n",
    "])\n",
    "\n",
    "row_xs = np.array([  # left and right x coordinates of each parking row\n",
    "    [456, 1447],\n",
    "    [540, 1465],\n",
    "    [540, 1465],\n",
    "    [445, 1465],\n",
    "    [540, 1465],\n",
    "    [540, 1465],\n",
    "    [438, 1465],\n",
    "    [433, 1465],\n",
    "])\n",
    "\n",
    "correct_xs = 20\n",
    "correct_ys = 10\n",
    "\n",
    "row_ys[:, 0] -= correct_ys\n",
    "row_xs[:, 0] -= correct_xs\n",
    "row_xs[:, 1] += correct_xs\n",
    "\n",
    "spot_width = 20  # Width of a spot in terms of pixel\n",
    "\n",
    "# Output a dict with coordinates of each box\n",
    "spot_dict = {}\n",
    "for (xl, xr), (yt, yb) in zip(row_xs, row_ys):\n",
    "    y_ave = (yt + yb) // 2\n",
    "    for xv in np.arange(xl, xr, spot_width):\n",
    "        spot_dict[(xv, yt, xv + spot_width, y_ave)] = -1\n",
    "        spot_dict[(xv, y_ave, xv + spot_width, yb)] = -1\n",
    "\n",
    "with open('spot_dict.pickle', 'wb') as f_obj:\n",
    "    pickle.dump(spot_dict, f_obj)\n"
   ]
  },
  {
   "source": [
    "# Display Bounding Boxes for Each Spot\n",
    "\n",
    "It is apparent that the bounding boxes are not perfectly aligned with the parking spot. This is due to the footage not being completely stable (shot via a drone). Any coordinate system developed on one frame will not be 100% match for another frame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box_images = draw_bounding_box(all_images[[0, 5], 0], 'spot_dict.pickle')\n",
    "show_images(bounding_box_images, bounding_box_images.shape[0], file_name='images/bounding_box.png')"
   ]
  },
  {
   "source": [
    "# Save The Spot Image for CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the very first frame to generate training data.\n",
    "save_spot_images(all_images[[0, ],:], 'spot_dict.pickle')"
   ]
  },
  {
   "source": [
    "# Load The Trained CNN Model\n",
    "\n",
    "The pre-trained model is ResNet50"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values\n",
    "MODEL = keras.models.load_model('spot_parked_detection_model')\n",
    "INPUT_SHAPE = (72, 72, 3)\n",
    "PRE_MODEL = keras.applications.ResNet50(include_top=False, input_shape=INPUT_SHAPE)\n",
    "PRE_PROCESS = keras.applications.resnet.preprocess_input\n",
    "OUTPUT_FRAMES_FOLDER = Path('data/output_frames')\n",
    "OUTPUT_VIDEO = Path('video/predicted.avi')"
   ]
  },
  {
   "source": [
    "# Functions Used to Make Prediction And Draw Empty Spots"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_image(original_image, spot_dict, input_shape):\n",
    "    \"\"\"Predict the labels of each spot on the given image.\n",
    "\n",
    "    The function first grabs all the spot images. Then run the prediction, which\n",
    "    involves four\n",
    "     steps:\n",
    "    1. Use the pre-trained model's pre-process function to process the input.\n",
    "    2. Use the pre-trained model to extract features (i.e. use the predict() method)\n",
    "    3. Use the trained model to predict the probabilities of each label.\n",
    "    4. Choose the index of the largest probability as the predicted label.\n",
    "\n",
    "    :param original_image: The unadulterated image. Prediction will be made\n",
    "         on this image.\n",
    "    :param spot_dict: A dictionary containing the coordinates of all the spot\n",
    "        bounding boxes in its keys.\n",
    "    :param input_shape: Shape of the input image. Must be of shape\n",
    "        (height, width, channel).\n",
    "    :return: The predicted label of all the spots on the image.\n",
    "    \"\"\"\n",
    "    all_spots = np.array(\n",
    "        [cv2.resize(original_image[y1:y2, x1:x2], input_shape[:-1]) for x1, y1, x2, y2 in spot_dict.keys()],\n",
    "    )\n",
    "    all_pred = np.argmax(\n",
    "        MODEL.predict(PRE_MODEL.predict(PRE_PROCESS(all_spots))),\n",
    "        axis=1,\n",
    "    )\n",
    "    return all_pred\n",
    "\n",
    "\n",
    "def draw_prediction(original_image, spot_dict, all_pred, alpha):\n",
    "    \"\"\"Draw the predicted empty parking spot rectangles on unadulterated image.\n",
    "\n",
    "    Empty spots are labeled as green rectangle overlays. Obstacles that cannot\n",
    "    be parked upon, such as trees, landscapes, and cart return stations are\n",
    "    labeled as blue rectangle overlays.\n",
    "\n",
    "    Also included in the drawing is a message of the current number of empty spaces.\n",
    "    Its accuracy depends on the model.\n",
    "\n",
    "    :param original_image: The unadulterated image to have its emtpy parking spott\n",
    "        highlighted.\n",
    "    :param spot_dict: A dictionary containing the coordinates of all the spot\n",
    "        bounding boxes in its keys.\n",
    "    :para all_pred: Return value from predict_on_image(). Must be of shape (n, )\n",
    "        where n is the number of total spots.\n",
    "    :param alpha: The level of transparency in the overlay. The higher the alpha,\n",
    "        the more opaque.\n",
    "    :return: The newly drawn images.\n",
    "    \"\"\"\n",
    "    new_image = np.copy(original_image)\n",
    "    overlay = np.copy(original_image)\n",
    "    # get the counts of all classes\n",
    "    counts = dict(zip(*np.unique(all_pred, return_counts=True)))\n",
    "    for pred, (x1, y1, x2, y2) in zip(all_pred, spot_dict.keys()):\n",
    "        if pred == 0:  # green for empty space\n",
    "            cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), -1)\n",
    "        elif pred == 1:  # blue for obstacle space\n",
    "            cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image)\n",
    "    cv2.putText(\n",
    "        new_image,\n",
    "        f'Available: {counts[0]} spots',\n",
    "        (30, 95),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7,\n",
    "        (0, 0, 0),\n",
    "        2,\n",
    "    )\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def predict_and_mark_empty_spot(\n",
    "    original_images,\n",
    "    spot_dict_pickle_name,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    alpha=0.3,\n",
    "):\n",
    "    \"\"\"Combination of predict_on_image() and darw_prediction().\n",
    "\n",
    "    A simple wrapper of the other two main functions mentioned earlier.\n",
    "\n",
    "    :param original_images: A np array of unadulterated original images.\n",
    "    :param spot_dict_pickle_name: The name of pickle file corresponding\n",
    "         to the `spot_dict`.\n",
    "    :param input_shape: Shape of the input image. Must be of shape\n",
    "        (height, width, channel). Default to the value INPUT_SHAPE\n",
    "    :param alpha: Transparency level on an image. The lower the alpha, the\n",
    "        more transparent. Allowed range is 0 to 1.\n",
    "    :return: A numpy object with each element being all the predicted labels.\n",
    "    \"\"\"\n",
    "    with open('spot_dict.pickle', 'rb') as f_obj:\n",
    "        spot_dict = pickle.load(f_obj)\n",
    "    all_preds = np.array([\n",
    "        predict_on_image(img, spot_dict, input_shape=input_shape) for img in original_images\n",
    "    ])\n",
    "    return np.array([\n",
    "        draw_prediction(img, spot_dict, pred, alpha=alpha) for img, pred in zip(original_images, all_preds)\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_predicted_video_frames(\n",
    "    pred_frequency: int,\n",
    "    input_video_path: Path,\n",
    "    output_frames_folder: Path,\n",
    "    spot_dict_pickle_name: str,\n",
    "    max_frame_count: int = 1000,\n",
    "):\n",
    "    \"\"\"Get raw frames from the original video, draw a new frame after making empty spot prediction.\n",
    "\n",
    "    After the new frame is drawn, it is saved as a PNG file.\n",
    "\n",
    "    :param pred_frequency: The frequency of making predictions and drawing a new frame. For\n",
    "        instance, if `pred_frequency` is 5, that means we make preidiction and draw a new frame\n",
    "        every 5 frames.\n",
    "    :param input_video_path: A Path object containing the path to the original video.\n",
    "    :param output_frames_folder: A Path obejct containing the path to the destination of the\n",
    "        new frame images.\n",
    "    :param spot_dict_pickle_name: Name of the pickle file for `spot_dict`.\n",
    "    :param max_fram_count: Maximum number of frames to capture from the original video. Default\n",
    "        to 1000. For a simple demo purpose, there is no need to grab all the frames in the original\n",
    "        video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(input_video_path))\n",
    "    ret, frame_count, predicted_count = True, 0, 0\n",
    "\n",
    "    while ret and predicted_count < max_frame_count:\n",
    "        ret, frame = cap.read()\n",
    "        frame_count += 1\n",
    "        if frame_count % pred_frequency == 0:\n",
    "            new_images = predict_and_mark_empty_spot(np.array([frame]), spot_dict_pickle_name)\n",
    "            file_name = f'{output_frames_folder}/{frame_count:04}.png'\n",
    "            cv2.imwrite(file_name, new_images[0])\n",
    "            predicted_count += 1\n",
    "            print(file_name)\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "def create_predicted_video(\n",
    "    output_frames_folder: Path,\n",
    "    output_video_path: Path,\n",
    "    video_length: int,\n",
    "    fps: int,\n",
    "):\n",
    "    \"\"\"Create a video by combining the newly drawn frames together.\n",
    "\n",
    "    :param output_frames_folder: A Path obejct containing the path to the destination of the\n",
    "        new frame images.\n",
    "    :param output_video_path: A Path object containing the path to the destination of the\n",
    "        new video.\n",
    "    :param video_length: The duration of the video, in seconds.\n",
    "    :param fps: Frame per second. Video appears more speeded up if a higher FPS is given.\n",
    "    \"\"\"\n",
    "    # must sort the paths because .glob does not guarantee order\n",
    "    all_frame_paths = sorted(OUTPUT_FRAMES_FOLDER.glob('*.png'))\n",
    "    all_frames = [cv2.imread(str(frame_path)) for frame_path in all_frame_paths[:video_length * fps]]\n",
    "    h, w, channels = all_frames[0].shape\n",
    "    size = (w, h)\n",
    "    out = cv2.VideoWriter(\n",
    "        str(output_video_path),\n",
    "        cv2.VideoWriter_fourcc(*'DIVX'),\n",
    "        fps,\n",
    "        size,\n",
    "    )\n",
    "    for i, frame in enumerate(all_frames[:]):\n",
    "        out.write(frame)\n",
    "        print(f'Frame {i:04} written.')\n",
    "    out.release()"
   ]
  },
  {
   "source": [
    "# Predict on Static Frame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_images = predict_and_mark_empty_spot(all_images[[0, 5], 0], 'spot_dict.pickle')\n",
    "show_images(predicted_images, predicted_images.shape[0], file_name='images/empty_spot_predicted.png')"
   ]
  },
  {
   "source": [
    "# Predict on Video (Not Live)\n",
    "\n",
    "The prediction is too slow for a live view. Thus, we have to save the prediction into a new video"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all the predicted frames\n",
    "get_predicted_video_frames(2, VIDEO_PATH, OUTPUT_FRAMES_FOLDER, 'spot_dict.pickle', max_frame_count=500)\n",
    "print('Frame prediction DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stich the predicted frames into a new video\n",
    "create_predicted_video(OUTPUT_FRAMES_FOLDER, OUTPUT_VIDEO, video_length=30, fps=12)\n",
    "print('Video CREATED!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}